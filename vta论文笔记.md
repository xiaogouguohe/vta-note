# 0 摘要

- 专用深度学习加速堆栈（DL）有缺陷，不灵活
- 提出VTA
  - 通过两层ISA和一个JIT编译器来实现灵活性
  - 两层ISA基于一个task-ISA和一个microcode-ISA（微码）
    - task-ISA协调并发计算和内存任务
    - 微码ISA实现多种操作，通过张量-张量运算来实现
- 一个运行时系统
  
  - 配备JIT编译器，可以灵活地生成和执行不同种类的代码，从而有效使用VTA架构
- VTA已经继承并开源到Apache TVM中

  - Apache TVM是一种深度学习编译堆栈


  - 我们构建这样一个流程，它可以提供在设计方面的探索，来生成用户自定义的硬件框架和可以被主流学习框架利用的软件操作库
  - 通过在FPGA上部署深度学习模型来证明我们这个方法

# 1 Introduction

- 硬件专业化是一种加速应用程序和工作负载的有力手段
- 但是，深度学习不是一个静态的领域，而是在不断变化
  - 机器学习社区会频繁改变
    - 写模型的软件
    - 它们的模型自身
    - 表述模型的语言
    - 它们操作的数据类型
- 研究主要集中在两种加速器的设计上
  - 固定功能加速器
  - 可编程加速器（领域专用加速器）
- 当前的解决方案可提供最佳性能，但无法融入不断发展的机器学习领域（硬件资源受限制）
  - 问题：为什么固定功能的加速器会受到硬件资源的限制，也就是说，可编程加速器的灵活性体现在哪里？？？为什么说为了达到最佳性能， 需要深度学习编译器，来把负载映射到硬件上？？？
- 可编程加速器利用ISA提供更大灵活性
  - 有可编程特性了，现在就要想办法达到最佳性能，就需要深度学习编译器
  - 这些加速器的自定义行为，高度依赖于透明和模块化软件堆栈的可用性

- 面临的一个挑战是，将专业创新和快速变化的机器学习软件联系起来

- 引入VTA
  - 一种经过明确编程的体系结构，与功能强大的JIT编译器和运行时配对，可以和深度学习模型一起演进，又不牺牲专业化的优势
- VTA的如下贡献
  - 可编程的加速器设计
    - 两级编程接口
      - 高级任务ISA，允许通过编译器堆栈进行显式任务调度
      - 低级微代码ISA，提供软件定义的操作灵活性
    - 可参数化
      - 可以自定义硬件内在函数，内存和数据类型，以适应后端要求
      - 什么叫做自定义硬件相关？？？
  - 一种用于异构执行的可扩展运行时系统
    - 该系统执行微代码内核的JIT编译
  - 调度自动调整平台
- 通过为两个边缘FPGA适配不同的工作负载，展示了VTA的灵活性
- fig.1
  - 使得给定的模型适应硬件后端，是谁适应谁？？？

# 2 VTA硬件软件栈预览

- 在VTA上运行端到端负载，需要完整的软件栈，来把高级模型映射到VTA公开的编程接口，这个映射是什么意思？？？映射到接口上，是否最后还是要映射到电路板上？？？
- 列出来整个VTA栈，它已经在Apache TVm深度学习编译堆栈里了
  - 深度学习框架
    - 如TensorFlow，PyTorch等等
    - TVM具有从这些深度学习框架中提取出模型的能力，这使得从框架到VTA的通用编译成为可能
  - Relay图优化器（Relay Graph Optimizer）
    - Relay是一门语言
      - 把被以前的框架和深度学习编译器使用的计算图概括成编程语言
    - 计算图是什么
      - 回想之前看过的神经网络模型
    - IR是什么
      - 中间表示层
      - 前端（深度学习框架/编程语言）到后端（硬件）的桥梁
      - 目前有很多中间表示层
  - TVM Operator Optimizer
    - 使得这样一个过程自动化：把负载调度到VTA加速器的variants（各个部分）
    - 为什么要进行调度
      - 平铺计算？？？
      - 线程并行
      - 运算符划分为多个子计算？？？
  - JIT编译和运行（从这里开始才是VTA）
    - 编译成二进制文件
    - 管理CPU和VTA之间的异构执行？？？
  - 硬件架构（3.1）
    - VTA是可参数化的加速器
    - 编译器堆栈使用两级编程接口显式编程，两级接口？？？
    - 参数化是什么？？？
    - 为什么参数化就可以把一套VTA设计定位到不同硬件？？？

# 3 VTA硬件架构和JIT Runtime

- 两个组件
  - VTA硬件体系结构
  - JIT编译器和运行时

## 3.1 硬件架构

- 架构图见fig.3
- VTA由四个模块组成
  - fetch
  - load
  - compute
  - store
- 任务流水线
  - 由上述模块定义
  - 流水线使得
    - 计算为主的负载友好
    - 内存为主的负载友好
- 模块之间通过命令队列和SRAM进行通信
- 内存访问的同步
  - 通过依赖关系队列（dependency queues）实现
  - 主要是防止写后读、读后写之类的数据冲突
- 只要依赖关系被适当管理，三阶段体系结构（负载计算存储）可用于构建任意深度的任务流水线

### 3.1.1 可参数化

- 可以修改GEMM张量（也就是矩阵乘法的那些矩阵）的形状，来影响硬件资源的利用率

  - 修改GEMM张量的输入，权重和累加器张量的形状，会直接影响要实例化的乘法器数量和SRAM端口的宽度

- 每种数据类型都可以自定义为不同精度

  - 比如权重和输入类型可以为8位或更少，累加类型可以为32位或更少

  - 这使得可以在资源受限的情况下扩展芯片的算术密度，算术密度是什么意思，是下面这个 链接吗？？？

    https://blog.csdn.net/weixin_43728590/article/details/107212069

### 3.1.2 对外暴露的任务级流水线并行性

- 任务级流水线并行化（TLPP）是VTA体系结构中的重要特性
  - 同时使用计算和内存资源，最大化它们的利用率（否则某些时候会有闲置）

- TLPP是基于访问-执行解耦的（也就是说访问和执行是不互相影响的）
  - 把任务划分成互斥的执行上下文，以便load（取出，取指）、计算、存储可以并发执行，不会互相干扰
  - 在TVM中，为了实现这样的划分，使用虚拟线程
- 为了保证解耦之后的访问-执行指令流能及时正确执行，将依赖信息编码为指令
  - 这有效地导致了内存延迟隐藏在计算绑定的工作负载上（如2d卷积）？？？
  - 内存延迟是因为，处理器（CPU）比内存的时钟周期快，导致很多个处理器时钟周期才对应上一个内存周期？？？

### 3.1.3 基于任务等级的ISA（指令集）

- VTA支持一个高层次的基于任务的ISA，它把多周期的计算和内存指令，包括LOAD、GEMM、ALU、STORE指令（fig.4），进行编码
- 四种指令分别做什么
  - LOAD和STORE指令描述了输入如何从DRAM取出来，并且存储到片上的SRAM
    - 支持对内存的交错访问，这使得可以在不修改内存布局的情况下，取出tensor tiles（矩阵的平铺？？？）
  - GEMM和ALU指令声明了微编码内核，这是基于微操作指令的
    - 微操作指令描述了数据访问的模式，这个模式定义了深度学习的操作符？？？
- 对VTA的执行流水线做如下简单描述（fig.3）
  - fetch模块从DRAM取出来任务指令，并且根据指令类型来把它们分发到相应的任务队列，这些任务队列连接着load，compute和store模块
  - load模块把DRAM的输入，权重，bias tensor tiles加载到片上内存
  - compute模块把DRAM的一个微编码内核加载到片上内存
    - 微编码内核是基于微操作的
    - 微操作描述了访问数据的模式（输入，权重和bias）
  - compute模块执行微编码内核，来实现：通过GEMM内核进行密集的线性代数运算，或通过ALU进行逻辑算术运算
  - store模块读取compute模块的结果，写回DRAM

### 3.1.4 Compute模块

- 两个函数单元对寄存器文件进行操作：张量ALU和GEMM核
- 张量ALU执行逐元素的张量（向量）操作，如加法、激活、规范化、池化任务
- GEMM核对输入和权重张量执行矩阵乘法，以实现常见的深度学习运算操作，如2D卷积、全连接层等

- GEMM核执行矩阵乘法运算的速率是，每周期执行一个输入权重矩阵的乘法
- ###
- TVM使用张量化：一种自动化方法，它将深度学习运算操作（如2d卷积）映射到固定张量硬件内在函数

### 3.1.5 微码ISA

- compute核从微操作缓存中读取指令
  - 这些指令描述了如何对数据进行计算
- fig.5详细描述了GEMM核如何对数据进行计算，这些数据存在输入、权重、累加存储器中

- 微操作不提供控制流，因此，指令需要被展开来表达可重复的数据模板？？？
- 有两种类型的计算微操作：ALU和GEMM操作
- 现在已经没有对控制流指令的需求（微操作指令不提供控制流），在此前提条件下，为了最大程度减少微操作内核的占用量，计算内核在在两层循环内执行微操作序列
  - 这个循环通过仿射函数计算每个张量寄存器的位置？？？

- 当发送到加速器时，这种压缩方法可以减少微操作的占用空间？？？

## 3.2 JIT运行时系统

- JIT运行时支持在CPU主机和加速期之间协同执行深度学习的工作负载
- 设计时遵循五个目标
  - 异构执行
  - 降低编译器设计复杂度
  - 克服物理限制
  - 减少二进制膨胀
  - 将来的证明？？？

### 3.2.1 异构执行

- 固定功能加速器的一个缺陷是模型演化，这些加速器大多数都是为固定模型而构造的
- 异构执行解决了这样的问题
  - 通过把运算适当调度到目标（如CPU或者VTA）
  - 例如CNN的第一卷积层，在CPU上表现良好
  - 原来是在哪执行这些运算？？？
- 异构执行还提供了一种回退机制，以支持VTA尚未支持的新的操作？？？

### 3.2.2 编译器设计

- 通过增加一个间接层，JIT不再需要去编写编译器代码生成的后端
  - 这些后端很繁琐，需要为不同的可编程加速器进行维护
- JIT编译器向TVM暴露了一个高层的API，来降低调度时间，抽象出了VTA对于不同变体的体系结构细节
  - 这使得我们能够扩展TVM编译器支持，本来这种支持是为了VTA而构建的，现在可以支持其它变体（类似于VTA的东西）

### 3.2.3 物理限制

- JIT运行时动态地生成和管理微内核
  - 它控制何时将内核从DRAM加载到微操作缓存（fig.3）中
  - 微操作缓存是受到加速器限制的
  - 这消除了微操作存储器的物理限制？？？
  - 允许我们支持大型模型，及时所有层的所有微操作内核都不能一次性放入SRAM（高速缓存存储）中？？？
  - 它还允许我们可以把原来是微操作缓存使用的区域换成其它，例如数据存储或计算单元？？？

### 3.2.4 二进制膨胀

- 把微内核的生成延迟到JIT编译阶段，可减少二进制膨胀
  - VTA的体系结构对控制流的支持有限，因此微内核必须被展开，而这会产生相当大的二进制数据
  - ###

### 3.2.5 将来的证明

###

# 4 VTA可继承的优化
## 4.1 不同FPGA尺寸的硬件探索
- 展示VTA架构灵活性的一种方法是根据不同的FPGA平台（做不同的适配）
- VTA设计提供了多个架构定制参数，如fig.1
  - 结构旋钮(Architectural knobs)包括
      - GEMM硬件内部形状
      - 数据类型
      - 张量ALU中并行运算单元的数量
      - ALU运算
      - 片上存储器之间的BRAM分布
    - 电路旋钮包括
      - 硬件流水线的程度，以关闭更高频率的定时，和锁相环频率
    - 这些定制旋钮定义了一个具有100到1000个单独设计的硬件设计空间，什么是硬件设计空间？？？
      - 在这个空间里探索VTA的优化
- 在假设计算资源利用率100%的情况下，基于理论吞吐率和频率，用峰值性能分析模型，对硬件设计进行初步过滤
  - 计算资源利用率通常无法达到百分之百
    - 窗口较大的conv2d之类的操作符，算术强度较高，利用率较高，接近峰值性能
    - conv2d（应该是普通的conv2d）运算，窗口大小为1，内存带宽受限
    - 任务级流水线可以减少性能损失
    
## 4.2 运算符调度的规划探索
- 调度自动调整
  - 一个过程，在这个过程中，自动搜索算法试图优化给定程序或工作负载以达到最佳性能
  - 通过应用不同的内存平铺，循环变换，矢量化张量化，并行化策略来执行自动调整
  - 然后使用TVM编译器来表示每个操作符（如conv2d，conv2d转置，group_conv2d等等）的掉多半
  - 使用TVM的自动调度库来获得调度
- 图6显示了为单个ResNet层优化不同VTA硬件候选时的自动调整搜索过程
  - XGBoost搜索算法，为每个硬件变量找到最佳的安排策略
  - 图6横轴是XGBoost试验次数，纵轴是吞吐量
  - 然后针对每个候选硬件调整每个工作负载的层？？？
  - 聚合推理时间用于选择最适合给定模型的VTA硬件变体？？？
- 在一个硬件上，对网络进行彻底调优要很久

## 4.3 一个优化的例子
- fig.7展示了一个分层优化的例子，是针对ResNet-18工作负载的分层优化，基于之前描述的硬件和schedule
  - 选择了8个有希望的硬件候选









问题

1. 实现一个硬件模块的全部流程

   - 编写verilog代码

   - 怎么具体部署到硬件上？需要先排列好电路元件和线路，然后等输入？
3. 用chisel实现和用verilog实现，是否是等价的？（和专用/可编程之间有没有必然联系）
4. 专用/可编程区别到底在哪？专用的就是写verilog在固定的板上运行，可编程的就可以在不同板子上运行吗？
4. 现在要实现一个模块的功能（例如3-8译码器），在具备不同资源的情况下，分别有哪些办法？
   - 只有不同种类的门电路（每种数量都可视为无限多）
     - 按照3-8译码器的电路结构搭建好电路
   - verilog
     - 写好的verilog代码，如何在硬件上发挥作用？
   - FPGA
     - 可编程的，可以用作不同功能（3-8译码器或者其它模块）
     - 需要做什么来实现这些不同的功能？
   - vta
     - 比FPGA的优势在哪里？
5. 前端和后端分别是什么？后端是电路板？
6. Relay是一门编程语言，把计算图表示成编程语言？
7. IR是中间表示层，有很多种，Relay是其中一种？
8. TVM调度是什么意思，TVM Operator Optimizer？
9. JIT编译是编译成二进制文件？
10. 为什么说要管理CPU和VTA的异构执行？
11. 负载是什么意思？计算绑定的负载和内存绑定的负载？
12. 访问-执行解耦是什么意思？
13. 依赖信息编码为指令是什么意思？
14. 内存延迟隐藏在计算绑定的工作负载上？内存延迟是因为，处理器（CPU）比内存的时钟周期快，导致很多个处理器时钟周期才对应上一个内存周期？
15. 微操作指令不提供控制流，指令和控制流有什么关系（x86-64）？？？
16. fig.3问题
    - LOAD模块是从DRAM读取到寄存器文件，那如何执行LOAD指令，数据流向应该是什么样子，是DRAM到寄存器文件的那条蓝色箭头吗？那么要读取的地址是从哪里流向DRAM的呢（处理器架构中是地址总线，csapp）？
    - LD->CMP几个橙色的队列是什么意思？
    - 为什么需要input buffer，weight buffer，直接读取到寄存器，compute模块直接从寄存器读取不可以吗？output buffer同理，直接写回寄存器，到时候从寄存器读出来不可以吗？或者这么说，输入和权重这些东西，可不可以直接存放在DRAM，也就是内存中，因为按照c语言的a + b，应该是直接从内存中获取a和b的？（这里要结合fig.5）
    - 为什么要微码的cache，指令直接从DRAM读取不可以吗？还是说这里存的不是高层ISA而是微码ISA，二者有所不同？
17. 两级ISA，为什么会两级？都是二进制编码
18. fig.5问题
    - 每个input buffer和weight buffer会取出来一个input矩阵和一个weight矩阵，扔进去GEMM进行乘法，出来一个累加矩阵，加到某个寄存器（注意一个矩阵加到一个寄存器）
    - 一条微指令的字段
    - 什么时候会调用微指令？高层指令执行到mul A, B这样的操作的时候？

19. 3.2.1当中的异构执行，把运算分配到CPU，原来是在哪里执行这些运算？ALU不也是CPU的一部分吗？
20. 异构执行的回退机制？

21. JIT编译器，为什么说不用编写编译器代码生成的后端？

22. 微操作内核是什么，为什么要放到SRAM（高速缓存存储）中？
23. VTA体系结构对控制流的支持有限，这里的控制流是什么意思？
24. 为什么说这个硬件架构是可参数化的？
25. 什么是结构旋钮和电路旋钮，是把提到的一些影响参数分类？这些参数是vta的还是FPGA的？这些参数都有哪些？硬件设计空间就是在这些参数调整的所有可能性的集合？
26. autotune，自动协调，是什么意思？
27. 图6是什么意思，调优到底是从哪几个方面调的？硬件的设计可能会影响vta的性能？运算符调度又能进行怎样的调整？

28. 论文和答辩重点不完全一样可不可以？



数据流的执行

- VTA依靠硬件模块之间的FIFO来使得并发任务的执行具有同步性
  - 这样的FIFO被称为依赖性FIFO
- 图展示了一个给定的硬件模块，如何从生产者到消费者同时执行
  - 通过FIFO和单读取/单写入SRAM缓冲区
  - 以数据流的方式
  - 每个模块通过写后读（RAW）和读后写（WAR）关系队列，连接到其使用者和生产者































